{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_summarizer(doc, n_sent):\n",
    "    '''\n",
    "    A simple extractive summarizer on the sentence level: just counting the frequencies of the words\n",
    "    in the documents and choose the most \"important sentences\" based on the number of most-occuring\n",
    "    words they contain\n",
    "    \n",
    "    Input:\n",
    "        doc: a spaCy document object of the text\n",
    "        n_sent: number of sentences to choose from the original text, \n",
    "                must be < the number of sentences in the text\n",
    "    \n",
    "    Output: a statistically summarized version of the original text\n",
    "    '''\n",
    "    import spacy \n",
    "    from spacy import displacy\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    from heapq import nlargest\n",
    "    sw = list(STOP_WORDS)\n",
    "    \n",
    "    #Build a frequency table\n",
    "    tokens = [token.text for token in doc]\n",
    "    freq = {}\n",
    "\n",
    "    for token in doc: \n",
    "        if token.text not in sw: \n",
    "            if token.text not in freq.keys():\n",
    "                freq[token.text] = 1\n",
    "            else: \n",
    "                freq[token.text] += 1\n",
    "                \n",
    "    max_freq = max(freq.values())\n",
    "    sentence_scores = {}\n",
    "    sentences = [sent for sent in doc.sents if len(sent.text.split(' ')) <= 20]\n",
    "    for sent in sentences: \n",
    "        for word in sent: \n",
    "            if word.text.lower() in freq.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = 1\n",
    "                else: \n",
    "                    sentence_scores[sent] += 1\n",
    "\n",
    "    summarized = nlargest(n_sent, sentence_scores, key=sentence_scores.get)\n",
    "    summarized = [sent.text for sent in summarized]\n",
    "    return ' '.join(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text: For a long time, core NLP techniques were dominated by machine-learning approaches that used linear models such as support vector machines or logistic regression, trained over very high dimensional yet very sparse feature vectors.\n",
      "Recently, the field has seen some success in switching from such linear models over sparse inputs to non-linear neural-network models over dense inputs. While most of the neural network techniques are easy to apply, sometimes as almost drop-in replacements of the old linear classifiers, there is in many cases a strong barrier of entry. In this tutorial I attempt to provide NLP practitioners (as well as newcomers) with the basic background, jargon, tools and methodology that will allow them to understand the principles behind the neural network models and apply them to their own work. This tutorial is expected to be self-contained, while presenting the different approaches under a unified notation and framework. It repeats a lot of material which is available elsewhere. It also points to external sources for more advanced topics when appropriate.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This tutorial is expected to be self-contained, while presenting the different approaches under a unified notation and framework. It also points to external sources for more advanced topics when appropriate.\\n It repeats a lot of material which is available elsewhere.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = '''For a long time, core NLP techniques were dominated by machine-learning approaches that used linear models such as support vector machines or logistic regression, trained over very high dimensional yet very sparse feature vectors.\n",
    "Recently, the field has seen some success in switching from such linear models over sparse inputs to non-linear neural-network models over dense inputs. While most of the neural network techniques are easy to apply, sometimes as almost drop-in replacements of the old linear classifiers, there is in many cases a strong barrier of entry. In this tutorial I attempt to provide NLP practitioners (as well as newcomers) with the basic background, jargon, tools and methodology that will allow them to understand the principles behind the neural network models and apply them to their own work. This tutorial is expected to be self-contained, while presenting the different approaches under a unified notation and framework. It repeats a lot of material which is available elsewhere. It also points to external sources for more advanced topics when appropriate.\n",
    "'''\n",
    "doc = nlp(doc)\n",
    "print('Full text:', doc)\n",
    "simple_summarizer(doc, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
